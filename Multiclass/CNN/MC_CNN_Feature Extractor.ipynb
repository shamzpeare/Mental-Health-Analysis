{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing**"
      ],
      "metadata": {
        "id": "gQ1Tr6U8yAKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y numpy gensim\n",
        "!pip install --no-cache-dir numpy==1.23.5 gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 845
        },
        "id": "632bzkYT3qrX",
        "outputId": "4be431f9-47e0-4e46-fa0a-d29504027005"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "\u001b[33mWARNING: Skipping gensim as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m154.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m135.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m168.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m112.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.3\n",
            "    Uninstalling scipy-1.15.3:\n",
            "      Successfully uninstalled scipy-1.15.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 2.3.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.6 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.3.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.3.2 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.23.5 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "58719aa216504bb7b189eb30dd629176"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQAvZThXv-cz",
        "outputId": "ec646e59-7074-45f7-f88d-0bdaf1bf2097"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import gensim.downloader as api\n",
        "from gensim.utils import simple_preprocess\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "\n",
        "word2vec_model = api.load(\"word2vec-google-news-300\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Combined Data.csv')\n",
        "def preprocess_and_tokenize(text):\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "\n",
        "    text = re.sub(r\"[^A-Za-z0-9\\s]\", \"\", text.lower())\n",
        "    tokens = simple_preprocess(text, deacc=True)\n",
        "    tokens = [word for word in tokens if word not in ENGLISH_STOP_WORDS]\n",
        "    return tokens\n",
        "\n",
        "df['tokens'] = df['statement'].apply(preprocess_and_tokenize)\n",
        "def get_average_word2vec(tokens, model, vector_size=300):\n",
        "    vectors = [model[word] for word in tokens if word in model]\n",
        "    return np.mean(vectors, axis=0) if vectors else np.zeros(vector_size)\n",
        "\n",
        "df['word2vec_vector'] = df['tokens'].apply(lambda x: get_average_word2vec(x, word2vec_model))\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "df['status'] = le.fit_transform(df['status'])\n",
        "\n",
        "label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "print(\"Label Mapping:\", label_mapping)\n",
        "\n",
        "print(df['status'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMMYWnkPwUdg",
        "outputId": "7733a723-9991-4058-d212-335102c347a8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label Mapping: {'Anxiety': 0, 'Bipolar': 1, 'Depression': 2, 'Normal': 3, 'Personality disorder': 4, 'Stress': 5, 'Suicidal': 6}\n",
            "status\n",
            "3    16351\n",
            "2    15404\n",
            "6    10653\n",
            "0     3888\n",
            "1     2877\n",
            "5     2669\n",
            "4     1201\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Prepare data\n",
        "X = np.stack(df['word2vec_vector'].values)\n",
        "y = df['status'].values\n",
        "\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
        "y_tensor = torch.tensor(y, dtype=torch.long).to(device)\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "train_data = TensorDataset(X_train, y_train)\n",
        "test_data = TensorDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "fZmkNHscywKR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABSnxssc8ECt",
        "outputId": "8b1ed81e-998b-462d-c010-66567b540200"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define CNN\n",
        "class CNNFeatureExtractor(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(CNNFeatureExtractor, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool(x).squeeze(-1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def extract_features(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool(x).squeeze(-1)\n",
        "        return x\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Dataset setup (X_train, y_train, X_test, y_test must already be defined and on correct device)\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Model, loss, optimizer\n",
        "num_classes = len(torch.unique(y_train))\n",
        "model = CNNFeatureExtractor(input_size=X_train.shape[1], num_classes=num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop with loss printing\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader.dataset)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Feature extraction for traditional classifiers\n",
        "model.eval()\n",
        "train_features, train_labels = [], []\n",
        "test_features, test_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        features = model.extract_features(inputs).cpu().numpy()\n",
        "        train_features.extend(features)\n",
        "        train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        features = model.extract_features(inputs).cpu().numpy()\n",
        "        test_features.extend(features)\n",
        "        test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Traditional classifiers\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "svm_classifier.fit(train_features, train_labels)\n",
        "svm_preds = svm_classifier.predict(test_features)\n",
        "print(f\"SVM Accuracy: {accuracy_score(test_labels, svm_preds):.2f}\")\n",
        "\n",
        "dt_classifier = DecisionTreeClassifier()\n",
        "dt_classifier.fit(train_features, train_labels)\n",
        "dt_preds = dt_classifier.predict(test_features)\n",
        "print(f\"Decision Tree Accuracy: {accuracy_score(test_labels, dt_preds):.2f}\")\n",
        "\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100)\n",
        "rf_classifier.fit(train_features, train_labels)\n",
        "rf_preds = rf_classifier.predict(test_features)\n",
        "print(f\"Random Forest Accuracy: {accuracy_score(test_labels, rf_preds):.2f}\")\n"
      ],
      "metadata": {
        "id": "_k7S0KFC0J58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29bbd00d-1b5c-4997-ae9d-b659011cc09f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.4266\n",
            "Epoch [2/10], Loss: 1.3511\n",
            "Epoch [3/10], Loss: 1.3206\n",
            "Epoch [4/10], Loss: 1.2924\n",
            "Epoch [5/10], Loss: 1.2722\n",
            "Epoch [6/10], Loss: 1.2575\n",
            "Epoch [7/10], Loss: 1.2444\n",
            "Epoch [8/10], Loss: 1.2334\n",
            "Epoch [9/10], Loss: 1.2247\n",
            "Epoch [10/10], Loss: 1.2163\n",
            "SVM Accuracy: 0.59\n",
            "Decision Tree Accuracy: 0.49\n",
            "Random Forest Accuracy: 0.61\n"
          ]
        }
      ]
    }
  ]
}